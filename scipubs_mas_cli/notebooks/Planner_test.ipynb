{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.11-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Downloading psycopg2_binary-2.9.11-cp312-cp312-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.7 MB 330.3 kB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.1/2.7 MB 751.6 kB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.2/2.7 MB 1.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.3/2.7 MB 1.6 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.5/2.7 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.7/2.7 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.0/2.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.4/2.7 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.6/2.7 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.0/2.7 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.2/2.7 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.6/2.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 4.6 MB/s eta 0:00:00\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.11\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "HTJmAK2_FO9y"
   },
   "outputs": [],
   "source": [
    "from typing import List, Literal, Optional, Dict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class PlannerStep(BaseModel):\n",
    "    step_id: str = Field(\n",
    "        ...,\n",
    "        description=\"Краткий идентификатор шага, например 'plan', 'classify', 'collect', 'analyze'\"\n",
    "    )\n",
    "    description: str = Field(\n",
    "        ...,\n",
    "        description=\"Человеко-читаемое описание шага\"\n",
    "    )\n",
    "    agent: Literal[\"Planner\", \"Classifier\", \"Collector\", \"Analyst\"] = Field(\n",
    "        ...,\n",
    "        description=\"Какой агент выполняет шаг\"\n",
    "    )\n",
    "    input_from: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"ID предыдущих шагов, чьи результаты используются на этом шаге\"\n",
    "    )\n",
    "\n",
    "\n",
    "class PlannerOutput(BaseModel):\n",
    "    # 1. Понимание задачи\n",
    "    user_intent: Literal[\n",
    "        \"describe_dataset\",          # просто описать, что есть в данных\n",
    "        \"trend_over_time\",          # динамика по годам / времени\n",
    "        \"compare_groups\",           # сравнение групп (например, стран, доменов)\n",
    "        \"top_k_entities\",           # топ-k авторов / журналов / стран и т.п.\n",
    "        \"correlation_or_relationship\",  # связь между параметрами (например, цитируемость vs год)\n",
    "        \"raw_table_view\",           # просто таблица (raw data / отфильтрованный срез)\n",
    "        \"other\",\n",
    "    ] = Field(\n",
    "        ...,\n",
    "        description=\"Высокоуровневый тип анализа, который запросил пользователь\"\n",
    "    )\n",
    "\n",
    "    question_rewrite: str = Field(\n",
    "        ...,\n",
    "        description=\"Переформулировка запроса пользователя в строгих технических терминах\"\n",
    "    )\n",
    "\n",
    "    # 2. Нужен ли Classifier и по какой оси группировать\n",
    "    requires_classifier: bool = Field(\n",
    "        ...,\n",
    "        description=\"Нужно ли вызывать Classifier для выбора конкретных значений параметров из словарей БД\"\n",
    "    )\n",
    "\n",
    "    classification_dimension: Optional[Literal[\n",
    "        \"domain\",\n",
    "        \"field\",\n",
    "        \"subfield\",\n",
    "        \"topic\",\n",
    "        \"title\",\n",
    "        \"publication_year\",\n",
    "        \"cited_by_count\",\n",
    "        \"journal\",\n",
    "        \"abstract\",\n",
    "        \"doi\",\n",
    "        \"none\",\n",
    "    ]] = Field(\n",
    "        None,\n",
    "        description=\"Список полей из таблицы articles, для которых Classifier должен выбрать подходящие значения из известных списков (словари допустимых значений)\"\n",
    "    )\n",
    "\n",
    "    # 3. Фильтры, которые Collector должен учесть в SQL\n",
    "    filters: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=(\n",
    "            \"Набор текстовых фильтров, которые нужно отразить в SQL \"\n",
    "            \"(годы, домены, страны, ограничения по цитируемости и т.п.)\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "     # 4. Инструкция для Collector’а: SQL-шаблон и параметры\n",
    "    collector_sql_template: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Параметризованный SQL-шаблон для Collector’а. \"\n",
    "            \"Используются только реальные поля таблицы articles: \"\n",
    "            \"id, title, doi, publication_year, abstract, citated_by_count, \"\n",
    "            \"journal, domain, field, subfield, topic. \"\n",
    "            \"Вместо конкретных значений – плейсхолдеры вида :start_year, :end_year, :topics и т.п.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    collector_parameters: Dict[str, str] = Field(\n",
    "        default_factory=dict,\n",
    "        description=(\n",
    "            \"Словарь параметр → описание. \"\n",
    "            \"Ключи – имена плейсхолдеров без двоеточия (например, 'start_year', 'topics'). \"\n",
    "            \"Значения – текстовое объяснение, ЧТО Classifier должен туда подставить \"\n",
    "            \"(например, 'Начальный год диапазона publication_year, выбранный из списка всех доступных годов').\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # 4.1 Явный список ожидаемых параметров (удобно для Classifier’а)\n",
    "    required_parameters: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=(\n",
    "            \"Список имён параметров (без двоеточия), которые должны быть заполнены \"\n",
    "            \"Classifier’ом перед выполнением SQL. \"\n",
    "            \"Например: ['start_year', 'end_year', 'topics'].\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # 5. Формат результата и задачи для Analyst’а\n",
    "    expected_result_format: Literal[\n",
    "        \"table\",\n",
    "        \"bar_chart\",\n",
    "        \"line_chart\",\n",
    "        \"histogram\",\n",
    "        \"text_summary\",\n",
    "        \"table_and_summary\",\n",
    "        \"chart_and_summary\",\n",
    "    ] = Field(\n",
    "        ...,\n",
    "        description=\"В каком виде Analyst должен представить итоговый результат\"\n",
    "    )\n",
    "\n",
    "    analyst_instructions: str = Field(\n",
    "        ...,\n",
    "        description=\"Чёткая инструкция для Analyst’а: что посчитать, как объяснить, что визуализировать\"\n",
    "    )\n",
    "\n",
    "    # 6. План шагов между агентами (для LangGraph / оркестратора)\n",
    "    steps: List[PlannerStep] = Field(\n",
    "        ...,\n",
    "        description=\"Декомпозиция задачи на шаги для Planner / Classifier / Collector / Analyst\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Literal, Union, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class MatchedValues(BaseModel):\n",
    "    column: Literal[\"publication_year\", \"domain\", \"field\", \"subfield\", \"topic\", \"journal\"]\n",
    "    values: List[Union[int, str]] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Список значений из словаря по этому столбцу, которые соответствуют запросу пользователя\"\n",
    "    )\n",
    "    rationale: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Краткое объяснение, почему выбраны именно эти значения (для отладки/логов)\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ClassifierOutput(BaseModel):\n",
    "    used_query: str = Field(\n",
    "        ...,\n",
    "        description=\"Фактический текст запроса, на основе которого принималось решение (можно взять question_rewrite)\"\n",
    "    )\n",
    "\n",
    "    # Параметры, которые будут подставлены в SQL-шаблон Collector'а\n",
    "    resolved_parameters: Dict[str, Union[int, str, List[Union[int, str]]]] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Словарь параметр → значение, которые нужно подставить в SQL-шаблон. \"\n",
    "            \"Ключи ДОЛЖНЫ совпадать с именами из PlannerOutput.required_parameters.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Для прозрачности – какие значения по каким полям были подобраны\n",
    "    matched_values: List[MatchedValues] = Field(\n",
    "        default_factory=list,\n",
    "        description=(\n",
    "            \"Список подобранных значений по каждому столбцу (publication_year/domain/field/subfield/topic/journal) \"\n",
    "            \"из известных словарей.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # На всякий случай – флаг уверенности\n",
    "    confidence: float = Field(\n",
    "        0.0,\n",
    "        description=\"Эвристическая оценка уверенности Classifier'а от 0 до 1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class ClassifierContext(BaseModel):\n",
    "    \"\"\"Контекст, который мы передаём в LLM-классификатор.\n",
    "\n",
    "    Здесь уже есть:\n",
    "      - запрос пользователя и переписанный вопрос от Planner'а,\n",
    "      - какие параметры нужно заполнить (required_parameters),\n",
    "      - по каким полям таблицы нужно подбирать значения (classification_dimensions),\n",
    "      - возможные значения по каждому полю (possible_*),\n",
    "      - SQL-шаблон и описание параметров Collector'а.\n",
    "    \"\"\"\n",
    "\n",
    "    user_query: str\n",
    "    question_rewrite: Optional[str]\n",
    "    required_parameters: List[str]\n",
    "    classification_dimensions: List[str]\n",
    "\n",
    "    possible_publication_years: List[int] = []\n",
    "    possible_domains: List[str] = []\n",
    "    possible_fields: List[str] = []\n",
    "    possible_subfields: List[str] = []\n",
    "    possible_topics: List[str] = []\n",
    "    possible_journals: List[str] = []\n",
    "\n",
    "    collector_sql_template: str\n",
    "    collector_parameters: Dict[str, str]\n",
    "\n",
    "\n",
    "def build_classifier_context(\n",
    "    user_query: str,\n",
    "    planner_output: PlannerOutput,\n",
    "    possible_values: Dict[str, list],\n",
    ") -> ClassifierContext:\n",
    "    \"\"\"Собрать ClassifierContext на основе PlannerOutput и словарей значений из БД.\"\"\"\n",
    "\n",
    "    # В текущей версии PlannerOutput у тебя есть одиночное поле classification_dimension.\n",
    "    # Для Classifier'а удобнее работать со списком, поэтому оборачиваем его в список,\n",
    "    # если оно не равно None/\"none\".\n",
    "    dims: List[str] = []\n",
    "    dim_single = getattr(planner_output, \"classification_dimension\", None)\n",
    "    if dim_single and dim_single != \"none\":\n",
    "        dims = [dim_single]\n",
    "\n",
    "    return ClassifierContext(\n",
    "        user_query=user_query,\n",
    "        question_rewrite=planner_output.question_rewrite,\n",
    "        required_parameters=planner_output.required_parameters,\n",
    "        classification_dimensions=dims,\n",
    "        possible_publication_years=possible_values.get(\"publication_year\", []),\n",
    "        possible_domains=possible_values.get(\"domain\", []),\n",
    "        possible_fields=possible_values.get(\"field\", []),\n",
    "        possible_subfields=possible_values.get(\"subfield\", []),\n",
    "        possible_topics=possible_values.get(\"topic\", []),\n",
    "        possible_journals=possible_values.get(\"journal\", []),\n",
    "        collector_sql_template=planner_output.collector_sql_template,\n",
    "        collector_parameters=planner_output.collector_parameters,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "\n",
    "def adapt_planner_json(raw_obj: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    obj: Dict[str, Any] = dict(raw_obj)\n",
    "\n",
    "    # Если это JSON Schema, а не план — явно падаем с понятной ошибкой\n",
    "    if \"properties\" in obj and \"user_intent\" not in obj and \"type\" in obj and obj.get(\"type\") == \"object\":\n",
    "        raise RuntimeError(\n",
    "            \"Модель вернула JSON Schema (описание PlannerOutput), \"\n",
    "            \"а не конкретный план. Нужно поправить system prompt: \"\n",
    "            \"запретить JSON Schema и попросить вернуть объект с заполненными полями.\"\n",
    "        )\n",
    "\n",
    "    # type -> user_intent (это для старого формата с полем \"type\": \"describe_dataset\" и т.п.)\n",
    "    if \"user_intent\" not in obj and \"type\" in obj:\n",
    "        obj[\"user_intent\"] = obj.pop(\"type\")\n",
    "\n",
    "    obj.setdefault(\"classification_dimension\", None)\n",
    "    obj.setdefault(\"filters\", [])\n",
    "    obj.setdefault(\"collector_parameters\", {})\n",
    "    obj.setdefault(\"steps\", [])\n",
    "\n",
    "    steps: List[Dict[str, Any]] = []\n",
    "    for step in obj[\"steps\"]:\n",
    "        step = dict(step)\n",
    "        if \"description\" not in step:\n",
    "            sid = step.get(\"step_id\", \"step\")\n",
    "            agent = step.get(\"agent\", \"Agent\")\n",
    "            step[\"description\"] = f\"Шаг '{sid}' выполняется агентом {agent}.\"\n",
    "        steps.append(step)\n",
    "\n",
    "    obj[\"steps\"] = steps\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Literal\n",
    "from psycopg2.extensions import connection as PGConnection\n",
    "\n",
    "# По каким колонкам умеем брать DISTINCT\n",
    "DistinctColumn = Literal[\"publication_year\", \"domain\", \"field\", \"subfield\", \"topic\", \"journal\"]\n",
    "\n",
    "\n",
    "class DBTool:\n",
    "    \"\"\"Обёртка над PostgreSQL для получения DISTINCT-значений\n",
    "    из таблицы с научными публикациями.\n",
    "\n",
    "    По умолчанию используем таблицу 'articles',\n",
    "    но при необходимости можно передать другое имя (например, 'articles').\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conn: PGConnection, table_name: str = \"publications\") -> None:\n",
    "        self.conn = conn\n",
    "        self.table_name = table_name\n",
    "\n",
    "    def list_distinct_values(self, column: DistinctColumn) -> List[Any]:\n",
    "        \"\"\"Вернуть список DISTINCT-значений для указанного столбца.\"\"\"\n",
    "        if column not in (\"publication_year\", \"domain\", \"field\", \"subfield\", \"topic\", \"journal\"):\n",
    "            raise ValueError(f\"Неподдерживаемый столбец для DISTINCT: {column}\")\n",
    "\n",
    "        order_clause = \"ORDER BY publication_year\" if column == \"publication_year\" else \"\"\n",
    "        sql = f\"SELECT DISTINCT {column} FROM {self.table_name} {order_clause};\"\n",
    "\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(sql)\n",
    "            rows = cur.fetchall()\n",
    "\n",
    "        return [row[0] for row in rows]\n",
    "\n",
    "    def load_all_distinct_values(self) -> Dict[str, List[Any]]:\n",
    "        \"\"\"Загрузить словари всех поддерживаемых колонок.\"\"\"\n",
    "        result: Dict[str, List[Any]] = {}\n",
    "        for col in (\"publication_year\", \"domain\", \"field\", \"subfield\", \"topic\", \"journal\"):\n",
    "            result[col] = self.list_distinct_values(col)  # type: ignore[arg-type]\n",
    "        return result\n",
    "\n",
    "\n",
    "def run_collector_query(conn: PGConnection, sql_template: str, params: Dict[str, Any]):\n",
    "    \"\"\"Простейший Collector: подставляет именованные параметры и выполняет SQL.\n",
    "\n",
    "    В Planner'е плейсхолдеры задаются как :param, а psycopg2 ожидает формат %(param)s,\n",
    "    поэтому здесь делаем минимальный препроцессинг.\n",
    "    \"\"\"\n",
    "\n",
    "    def convert_placeholders(sql: str) -> str:\n",
    "        res = sql\n",
    "        for name in params.keys():\n",
    "            res = res.replace(f\":{name}\", f\"%({name})s\")\n",
    "        return res\n",
    "\n",
    "    sql = convert_placeholders(sql_template)\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(sql, params)\n",
    "        rows = cur.fetchall()\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "r_eXJV8uEw0z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from pydantic import ValidationError\n",
    "\n",
    "BASE_URL = os.getenv(\"LITELLM_BASE_URL\", \"http://a6k2.dgx:34000/v1\")\n",
    "API_KEY = os.getenv(\"LITELLM_API_KEY\", \"sk-1yvtuYMQN37uRpXQe44qrA\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"qwen3-32b\")\n",
    "\n",
    "\n",
    "\n",
    "PLANNER_SYSTEM_PROMPT = \"\"\"Вы – агент Planner в многоагентной системе анализа научных публикаций.\n",
    "\n",
    "В системе есть следующие агенты:\n",
    "- Planner (вы): анализируете запрос пользователя, определяете цель, выбираете тип анализа и формируете план.\n",
    "- Classifier: по вашему указанию определяет категории/кластеры (например, domain, field, subfield, topic) или дополняет метаданные публикаций.\n",
    "- Collector: строит SQL-запросы к базе данных и извлекает нужные агрегированные данные.\n",
    "- Analyst: на основе данных от Collector строит визуализации (графики, таблицы) и пишет текстовый аналитический вывод.\n",
    "\n",
    "У нас есть PostgreSQL-БД с публикациями. В таблице articles содержатся, как минимум, следующие поля\n",
    "(вы можете использовать только их, не придумывайте несуществующие столбцы):\n",
    "- id\n",
    "- title\n",
    "- doi\n",
    "- publication_year\n",
    "- abstract\n",
    "- citated_by_count\n",
    "- journal\n",
    "- domain\n",
    "- field\n",
    "- subfield\n",
    "- topic\n",
    "(если для ответа нужны другие поля, формулируйте это как желание, но НЕ используйте их в SQL-шаблоне).\n",
    "\n",
    "ВАЖНО:\n",
    "- Ваша задача – СДЕЛАТЬ ПЛАН, а НЕ выполнять SQL и не строить графики.\n",
    "- Вы всегда возвращаете строго один JSON-объект, соответствующий схеме PlannerOutput.\n",
    "- НЕ добавляйте пояснений, комментариев, маркдаун или текст вне JSON.\n",
    "- НЕ НУЖНО возвращать JSON Schema или описание модели.\n",
    "- НУЖНО вернуть КОНКРЕТНЫЙ ПЛАН с заполненными значениями полей PlannerOutput.\n",
    "\n",
    "\n",
    "1. Сначала определите user_intent:\n",
    "   - описать данные (describe_dataset),\n",
    "   - изучить тренд по годам (trend_over_time),\n",
    "   - сравнить группы (compare_groups),\n",
    "   - найти топ-k сущностей (top_k_entities),\n",
    "   - изучить связь между величинами (correlation_or_relationship),\n",
    "   - получить просто отфильтрованную таблицу (raw_table_view),\n",
    "   - либо другое (other).\n",
    "\n",
    "2. Сформулируйте question_rewrite – точную техническую постановку задачи.\n",
    "\n",
    "3. Решите, нужен ли Classifier (requires_classifier):\n",
    "   - Если запрос затрагивает годы, домены, поля, подобласти, темы, журналы или другие сущности, которые должны быть выбраны из словарей допустимых значений – ставьте requires_classifier = true.\n",
    "   - Если можно обойтись без выбора по словарю (например, чистая агрегация по всем публикациям без фильтров) – можно requires_classifier = false.\n",
    "\n",
    "4. Заполните classification_dimensions:\n",
    "   - Укажите, по каким полям таблицы articles Classifier должен подбирать значения: publication_year, domain, field, subfield, topic, journal, cited_by_count.\n",
    "   - Например, если нужно выбрать годы 2018–2022 и темы, связанные с 'graph neural networks', то:\n",
    "     classification_dimensions = [\"publication_year\", \"topic\"].\n",
    "\n",
    "5. Заполните filters – текстовое описание фильтров на русском (без конкретных технических деталей SQL), например:\n",
    "   - \"Годы публикаций от 2018 до 2022 включительно\"\n",
    "   - \"Темы, связанные с graph neural networks\"\n",
    "   - \"Публикации в подобластях computer vision и natural language processing\"\n",
    "\n",
    "6. Составьте collector_sql_template:\n",
    "   - Используйте ТОЛЬКО таблицу articles и поля, перечисленные выше.\n",
    "   - НЕ подставляйте конкретные значения, только плейсхолдеры вида :start_year, :end_year, :topics, :domains и т.п.\n",
    "   - Если вы ожидаете, что Classifier вернёт список значений, используйте IN (:topics), IN (:subfields), IN (:domains).\n",
    "   - Пример: \n",
    "     SELECT publication_year, COUNT(*) AS n_papers\n",
    "     FROM articles\n",
    "     WHERE publication_year BETWEEN :start_year AND :end_year\n",
    "       AND topic IN (:topics)\n",
    "     GROUP BY publication_year\n",
    "     ORDER BY publication_year;\n",
    "\n",
    "7. Заполните collector_parameters:\n",
    "   - Ключ = имя плейсхолдера без двоеточия (start_year, end_year, topics).\n",
    "   - Значение = описание того, ЧТО должен подставить Classifier.\n",
    "   - Пример:\n",
    "     {\n",
    "       \"start_year\": \"Начальный год диапазона publication_year, выбранный Classifier'ом\",\n",
    "       \"end_year\": \"Конечный год диапазона publication_year, выбранный Classifier'ом\",\n",
    "       \"topics\": \"Список значений поля topic, выбранный Classifier'ом для описания тематики 'graph neural networks'\"\n",
    "     }\n",
    "\n",
    "8. Заполните required_parameters:\n",
    "   - Список всех имён параметров, которые должны быть заполнены перед выполнением SQL.\n",
    "   - Например: [\"start_year\", \"end_year\", \"topics\"].\n",
    "\n",
    "9. Определите expected_result_format:\n",
    "   - \"table\" – если пользователю нужна только таблица;\n",
    "   - \"bar_chart\" или \"line_chart\" – если речь о сравнении или динамике;\n",
    "   - \"histogram\" – если важно распределение;\n",
    "   - \"text_summary\" – чисто текстовый ответ;\n",
    "   - \"table_and_summary\" или \"chart_and_summary\" – комбинированные варианты.\n",
    "\n",
    "10. Заполните analyst_instructions:\n",
    "   - Опишите, какие именно величины посчитать и как интерпретировать результат.\n",
    "   - Если нужен график, явно укажите, какие поля по осям X и Y и что считать сериями.\n",
    "\n",
    "11. Заполните steps:\n",
    "   - Опишите цепочку действий от Planner к другим агентам, указывая agent для каждого шага.\n",
    "   - Используйте step_id вроде \"plan\", \"classify\", \"collect\", \"analyze\".\n",
    "   - В input_from указывайте список step_id предыдущих шагов, на которые опирается текущий шаг.\n",
    "\n",
    "ЕЩЁ РАЗ: ответ должен быть СТРОГО одним JSON-объектом без пояснений.\n",
    "\n",
    "Пример НЕПРАВИЛЬНОГО ответа (так делать нельзя):\n",
    "\n",
    "{\n",
    "  \"title\": \"PlannerOutput\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"user_intent\": { \"type\": \"string\", \"enum\": [...] },\n",
    "    ...\n",
    "  },\n",
    "  \"required\": [...]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from pydantic import ValidationError\n",
    "\n",
    "CLASSIFIER_SYSTEM_PROMPT = \"\"\"Вы – агент Classifier в многоагентной системе анализа научных публикаций.\n",
    "\n",
    "Входные данные, которые вам даёт Planner и окружающая система:\n",
    "- Исходный запрос пользователя (user_query).\n",
    "- Техническая переформулировка запроса (question_rewrite).\n",
    "- Список полей таблицы articles, по которым нужно подобрать значения (classification_dimensions):\n",
    "  publication_year, domain, field, subfield, topic, journal.\n",
    "- Список параметров, которые нужно заполнить (required_parameters), например:\n",
    "  ['start_year', 'end_year', 'topics'].\n",
    "- Для каждого поля из classification_dimensions – список допустимых значений:\n",
    "  - possible_publication_years: список целых чисел;\n",
    "  - possible_domains: список строк;\n",
    "  - possible_fields: список строк;\n",
    "  - possible_subfields: список строк;\n",
    "  - possible_topics: список строк;\n",
    "  - possible_journals: список строк.\n",
    "- SQL-шаблон Collector'а (collector_sql_template) и словарь описаний параметров (collector_parameters).\n",
    "\n",
    "ВАЖНО:\n",
    "- Вы НЕ пишете SQL и НЕ меняете SQL-шаблон.\n",
    "- Ваша задача – заполнить параметры, которые указаны в required_parameters, используя ТОЛЬКО значения из переданных списков.\n",
    "- Ответ должен быть СТРОГО одним JSON-объектом, соответствующим схеме ClassifierOutput.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def call_classifier_llm(context: ClassifierContext) -> ClassifierOutput:\n",
    "    \"\"\"Вызов LLM-классификатора.\n",
    "\n",
    "    context – объект ClassifierContext с:\n",
    "      - user_query, question_rewrite,\n",
    "      - required_parameters, classification_dimensions,\n",
    "      - possible_* словарями значений,\n",
    "      - collector_sql_template и collector_parameters.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    user_content = json.dumps(context.model_dump(), ensure_ascii=False)\n",
    "\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": CLASSIFIER_SYSTEM_PROMPT},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Ниже дан контекст для классификации в формате JSON. \"\n",
    "                    \"Используйте его, чтобы заполнить параметры согласно инструкции.\\\\n\\\\n\"\n",
    "                    + user_content\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "        \"response_format\": {\"type\": \"json_object\"},\n",
    "        \"temperature\": 0.1,\n",
    "    }\n",
    "\n",
    "    resp = requests.post(url, headers=headers, json=payload, timeout=100)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    # Пытаемся сначала взять reasoning_content, если модель его заполняет\n",
    "    message = data[\"choices\"][0][\"message\"]\n",
    "    raw = message.get(\"reasoning_content\") or message.get(\"content\")\n",
    "    if raw is None:\n",
    "        raise RuntimeError(f\"В ответе нет ни 'reasoning_content', ни 'content': {data}\")\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(raw)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise RuntimeError(f\"Classifier вернул невалидный JSON: {e}\\\\nRaw: {raw!r}\")\n",
    "\n",
    "    try:\n",
    "        return ClassifierOutput.model_validate(obj)\n",
    "    except ValidationError as e:\n",
    "        raise RuntimeError(f\"ClassifierOutput не прошёл валидацию Pydantic: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def call_planner_llm(user_query: str) -> PlannerOutput:\n",
    "    \"\"\"\n",
    "    Вызывает LLM (Qwen3-32B через LiteLLM/OpenAI-совместимый endpoint),\n",
    "    возвращает объект PlannerOutput.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": PLANNER_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_query},\n",
    "        ],\n",
    "        # Просим модель вернуть строго JSON-объект\n",
    "        \"response_format\": {\"type\": \"json_object\"},\n",
    "        \"temperature\": 0.1,\n",
    "    }\n",
    "\n",
    "    resp = requests.post(url, headers=headers, json=payload, timeout=100)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    # >>> ключевая строка: берём reasoning_content\n",
    "    raw = data[\"choices\"][0][\"message\"].get(\"reasoning_content\")\n",
    "    if raw is None:\n",
    "        raise RuntimeError(f\"В ответе нет поля 'reasoning_content': {data}\")\n",
    "\n",
    "    # reasoning_content — это строка с JSON\n",
    "    import json\n",
    "    try:\n",
    "        obj = json.loads(raw)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise RuntimeError(f\"Невалидный JSON в reasoning_content: {e}\\nRaw: {raw!r}\")\n",
    "\n",
    "    # адаптер к вашей схеме, см. ниже\n",
    "    obj = adapt_planner_json(obj)\n",
    "\n",
    "    #print('проверка обжекта:')\n",
    "    #print(obj)\n",
    "\n",
    "    try:\n",
    "        return PlannerOutput.model_validate(obj)\n",
    "    except ValidationError as e:\n",
    "        raise RuntimeError(f\"PlannerOutput не прошёл валидацию Pydantic: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_QUERIES = [\n",
    "    # describe_dataset\n",
    "    \"Опиши, какие данные есть в нашей базе научных публикаций: диапазон лет, \"\n",
    "    \"основные домены и поля, какие журналы и конференции встречаются чаще всего.\",\n",
    "\n",
    "    # trend_over_time\n",
    "    \"Построй динамику числа публикаций по теме deep learning с 2010 по 2024 год \"\n",
    "    \"и прокомментируй тренды.\",\n",
    "\n",
    "    # compare_groups\n",
    "    \"Сравни динамику числа публикаций по computer vision и natural language processing \"\n",
    "    \"с 2021 по 2024 год и выведи их на одном графике.\",\n",
    "\n",
    "    # top_k_entities\n",
    "    #\"Найди топ-20 авторов по количеству публикаций по теме transformers и выведи таблицу \"\n",
    "    #\"с числом статей и средним числом цитирований.\",\n",
    "\n",
    "    # correlation_or_relationship\n",
    "    \"Исследуй связь между годом публикации и числом цитирований для статей по теме \"\n",
    "    \"deep learning: падает ли цитируемость у более новых статей?\",\n",
    "\n",
    "    # raw_table_view\n",
    "    \"Покажи таблицу статей по теме graph neural networks, опубликованных с 2018 по 2022 год\",\n",
    "\n",
    "    # other (сложный комбинированный)\n",
    "    #\"Определи ключевые темы (topics) внутри домена artificial intelligence, \"\n",
    "    #\"построй по ним динамику числа публикаций за период 2000–2024 и оцени, \"\n",
    "    #\"какие темы сейчас растут быстрее всего.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "База данных papers_db уже существует.\n",
      "Таблица articles готова.\n",
      "Колонки domain, field, subfield, topic добавлены (если их не было).\n",
      "Добавлено/обновлено 56 записей.\n",
      "Первые строки в таблице:\n",
      "(1, '10.1000/xyz123', 'A Novel Iterative Method for Sparse Linear Systems', 2020, 35, 'Journal of Computational Mathematics', 'Mathematics', 'Applied Mathematics', 'Numerical Linear Algebra', 'Sparse Linear Systems')\n",
      "(2, '10.1016/j.apm.2021.005', 'Constrained Optimization Methods in Deep Learning', 2021, 52, 'Applied Mathematical Modelling', 'Computer Science', 'Artificial Intelligence', 'Optimization for Deep Learning', 'Constrained Optimization')\n",
      "(3, '10.1145/3394486.3403211', 'Graph-Based Recommender Systems at Scale', 2019, 120, 'ACM Transactions on Information Systems', 'Computer Science', 'Information Retrieval', 'Recommender Systems', 'Graph-Based Recommendation')\n",
      "(4, '10.1109/TIT.2022.3141234', 'Error-Correcting Codes for Fading Channels', 2022, 18, 'IEEE Transactions on Information Theory', 'Engineering', 'Electrical Engineering', 'Coding Theory', 'Error-Correcting Codes')\n",
      "(5, '10.1093/biomet/asab012', 'Bayesian Models for High-Dimensional Time Series', 2021, 44, 'Biometrika', 'Statistics', 'Bayesian Statistics', 'Time Series Analysis', 'Bayesian Time Series')\n",
      "(6, '10.1137/19M1276416', 'Fast Algorithms for Large-Scale Eigenvalue Problems', 2019, 67, 'SIAM Journal on Scientific Computing', 'Mathematics', 'Numerical Analysis', 'Eigenvalue Problems', 'Large-Scale Eigenproblems')\n",
      "(7, '10.1234/sim.2006.001', 'Simulated Study 1 on Machine Learning', 2006, 7, 'Journal of Machine Learning Research', 'Computer Science', 'Artificial Intelligence', 'Machine Learning', 'Supervised and Unsupervised Learning')\n",
      "(8, '10.1234/sim.2007.002', 'Simulated Study 2 on Numerical Analysis', 2007, 14, 'Numerical Algorithms', 'Mathematics', 'Applied Mathematics', 'Numerical Analysis', 'Numerical Methods')\n",
      "(9, '10.1234/sim.2008.003', 'Simulated Study 3 on Graph Theory', 2008, 21, 'Discrete Mathematics', 'Mathematics', 'Discrete Mathematics', 'Graph Theory', 'Graph Algorithms')\n",
      "(10, '10.1234/sim.2009.004', 'Simulated Study 4 on Optimization', 2009, 28, 'Optimization Letters', 'Mathematics', 'Optimization', 'Convex and Nonlinear Optimization', 'Optimization Methods')\n",
      "(11, '10.1234/sim.2010.005', 'Simulated Study 5 on Data Mining', 2010, 35, 'Data Mining and Knowledge Discovery', 'Computer Science', 'Data Science', 'Data Mining', 'Pattern Discovery')\n",
      "(12, '10.1234/sim.2011.006', 'Simulated Study 6 on Computer Vision', 2011, 42, 'IEEE Transactions on Neural Networks', 'Computer Science', 'Artificial Intelligence', 'Computer Vision', 'Image Recognition')\n",
      "(13, '10.1234/sim.2012.007', 'Simulated Study 7 on Reinforcement Learning', 2012, 49, 'Pattern Recognition Letters', 'Computer Science', 'Artificial Intelligence', 'Reinforcement Learning', 'Sequential Decision Making')\n",
      "(14, '10.1234/sim.2013.008', 'Simulated Study 8 on Cryptography', 2013, 56, 'Journal of Cryptology', 'Computer Science', 'Security', 'Cryptography', 'Modern Cryptographic Protocols')\n",
      "(15, '10.1234/sim.2014.009', 'Simulated Study 9 on Complex Networks', 2014, 63, 'Network Science', 'Physics', 'Statistical Physics', 'Complex Networks', 'Network Dynamics')\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# ---- Параметры подключения к серверу PostgreSQL ----\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"postgres\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "TARGET_DB = \"papers_db\"\n",
    "\n",
    "\n",
    "# ---------- 1. Создать БД, если её ещё нет ----------\n",
    "def ensure_database():\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"postgres\",\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD,\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT,\n",
    "    )\n",
    "    conn.autocommit = True\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT 1 FROM pg_database WHERE datname = %s;\", (TARGET_DB,))\n",
    "        exists = cur.fetchone() is not None\n",
    "        if not exists:\n",
    "            print(f\"Создаю базу данных {TARGET_DB}...\")\n",
    "            cur.execute(f\"CREATE DATABASE {TARGET_DB};\")\n",
    "        else:\n",
    "            print(f\"База данных {TARGET_DB} уже существует.\")\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# ---------- 2. Подключение к целевой БД ----------\n",
    "def get_target_connection():\n",
    "    return psycopg2.connect(\n",
    "        dbname=TARGET_DB,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD,\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT,\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------- 3. Таблица articles ----------\n",
    "def create_table(conn):\n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS articles (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        doi VARCHAR(255) UNIQUE NOT NULL,\n",
    "        abstract TEXT,\n",
    "        title TEXT NOT NULL,\n",
    "        publication_year INT,\n",
    "        cted_by_count INT,\n",
    "        journal TEXT,\n",
    "        domain TEXT,\n",
    "        field TEXT,\n",
    "        subfield TEXT,\n",
    "        topic TEXT\n",
    "    );\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(create_table_sql)\n",
    "    conn.commit()\n",
    "    print(\"Таблица articles готова.\")\n",
    "\n",
    "\n",
    "# ---------- 3a. Гарантированно добавить новые колонки (на случай старой версии таблицы) ----------\n",
    "def migrate_articles_add_new_columns(conn):\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            ALTER TABLE articles\n",
    "                ADD COLUMN IF NOT EXISTS domain   TEXT,\n",
    "                ADD COLUMN IF NOT EXISTS field    TEXT,\n",
    "                ADD COLUMN IF NOT EXISTS subfield TEXT,\n",
    "                ADD COLUMN IF NOT EXISTS topic    TEXT;\n",
    "        \"\"\")\n",
    "    conn.commit()\n",
    "    print(\"Колонки domain, field, subfield, topic добавлены (если их не было).\")\n",
    "    \n",
    "# ---------- 4. Заполнение / обновление данных ----------\n",
    "def insert_sample_data(conn):\n",
    "    # Базовые записи\n",
    "    base_rows = [\n",
    "        (\n",
    "            \"10.1000/xyz123\",\n",
    "            \"We propose a new numerical method for solving large sparse linear systems arising in scientific computing.\",\n",
    "            \"A Novel Iterative Method for Sparse Linear Systems\",\n",
    "            2020,\n",
    "            35,\n",
    "            \"Journal of Computational Mathematics\",\n",
    "            \"Mathematics\",\n",
    "            \"Applied Mathematics\",\n",
    "            \"Numerical Linear Algebra\",\n",
    "            \"Sparse Linear Systems\",\n",
    "        ),\n",
    "        (\n",
    "            \"10.1016/j.apm.2021.005\",\n",
    "            \"The paper studies optimization algorithms for training deep neural networks with constraints.\",\n",
    "            \"Constrained Optimization Methods in Deep Learning\",\n",
    "            2021,\n",
    "            52,\n",
    "            \"Applied Mathematical Modelling\",\n",
    "            \"Computer Science\",\n",
    "            \"Artificial Intelligence\",\n",
    "            \"Optimization for Deep Learning\",\n",
    "            \"Constrained Optimization\",\n",
    "        ),\n",
    "        (\n",
    "            \"10.1145/3394486.3403211\",\n",
    "            \"We introduce an efficient graph-based recommendation system and evaluate it on real-world datasets.\",\n",
    "            \"Graph-Based Recommender Systems at Scale\",\n",
    "            2019,\n",
    "            120,\n",
    "            \"ACM Transactions on Information Systems\",\n",
    "            \"Computer Science\",\n",
    "            \"Information Retrieval\",\n",
    "            \"Recommender Systems\",\n",
    "            \"Graph-Based Recommendation\",\n",
    "        ),\n",
    "        (\n",
    "            \"10.1109/TIT.2022.3141234\",\n",
    "            \"This work analyzes error-correcting codes for modern communication channels with fading.\",\n",
    "            \"Error-Correcting Codes for Fading Channels\",\n",
    "            2022,\n",
    "            18,\n",
    "            \"IEEE Transactions on Information Theory\",\n",
    "            \"Engineering\",\n",
    "            \"Electrical Engineering\",\n",
    "            \"Coding Theory\",\n",
    "            \"Error-Correcting Codes\",\n",
    "        ),\n",
    "        (\n",
    "            \"10.1093/biomet/asab012\",\n",
    "            \"We discuss Bayesian approaches to modeling high-dimensional time series in econometrics.\",\n",
    "            \"Bayesian Models for High-Dimensional Time Series\",\n",
    "            2021,\n",
    "            44,\n",
    "            \"Biometrika\",\n",
    "            \"Statistics\",\n",
    "            \"Bayesian Statistics\",\n",
    "            \"Time Series Analysis\",\n",
    "            \"Bayesian Time Series\",\n",
    "        ),\n",
    "        (\n",
    "            \"10.1137/19M1276416\",\n",
    "            \"The article explores fast algorithms for large-scale eigenvalue problems in scientific computing.\",\n",
    "            \"Fast Algorithms for Large-Scale Eigenvalue Problems\",\n",
    "            2019,\n",
    "            67,\n",
    "            \"SIAM Journal on Scientific Computing\",\n",
    "            \"Mathematics\",\n",
    "            \"Numerical Analysis\",\n",
    "            \"Eigenvalue Problems\",\n",
    "            \"Large-Scale Eigenproblems\",\n",
    "        ),\n",
    "    ]\n",
    "    # Метаданные по тематикам\n",
    "    topic_meta = {\n",
    "        \"Machine Learning\": (\n",
    "            \"Computer Science\",\n",
    "            \"Artificial Intelligence\",\n",
    "            \"Machine Learning\",\n",
    "            \"Supervised and Unsupervised Learning\",\n",
    "        ),\n",
    "        \"Numerical Analysis\": (\n",
    "            \"Mathematics\",\n",
    "            \"Applied Mathematics\",\n",
    "            \"Numerical Analysis\",\n",
    "            \"Numerical Methods\",\n",
    "        ),\n",
    "        \"Graph Theory\": (\n",
    "            \"Mathematics\",\n",
    "            \"Discrete Mathematics\",\n",
    "            \"Graph Theory\",\n",
    "            \"Graph Algorithms\",\n",
    "        ),\n",
    "        \"Optimization\": (\n",
    "            \"Mathematics\",\n",
    "            \"Optimization\",\n",
    "            \"Convex and Nonlinear Optimization\",\n",
    "            \"Optimization Methods\",\n",
    "        ),\n",
    "        \"Data Mining\": (\n",
    "            \"Computer Science\",\n",
    "            \"Data Science\",\n",
    "            \"Data Mining\",\n",
    "            \"Pattern Discovery\",\n",
    "        ),\n",
    "        \"Computer Vision\": (\n",
    "            \"Computer Science\",\n",
    "            \"Artificial Intelligence\",\n",
    "            \"Computer Vision\",\n",
    "            \"Image Recognition\",\n",
    "        ),\n",
    "        \"Reinforcement Learning\": (\n",
    "            \"Computer Science\",\n",
    "            \"Artificial Intelligence\",\n",
    "            \"Reinforcement Learning\",\n",
    "            \"Sequential Decision Making\",\n",
    "        ),\n",
    "        \"Cryptography\": (\n",
    "            \"Computer Science\",\n",
    "            \"Security\",\n",
    "            \"Cryptography\",\n",
    "            \"Modern Cryptographic Protocols\",\n",
    "        ),\n",
    "        \"Complex Networks\": (\n",
    "            \"Physics\",\n",
    "            \"Statistical Physics\",\n",
    "            \"Complex Networks\",\n",
    "            \"Network Dynamics\",\n",
    "        ),\n",
    "        \"Statistical Inference\": (\n",
    "            \"Statistics\",\n",
    "            \"Statistical Inference\",\n",
    "            \"Parametric and Nonparametric Inference\",\n",
    "            \"Estimation and Hypothesis Testing\",\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    topics = list(topic_meta.keys())\n",
    "\n",
    "    journals = [\n",
    "        \"Journal of Machine Learning Research\",\n",
    "        \"Numerical Algorithms\",\n",
    "        \"Discrete Mathematics\",\n",
    "        \"Optimization Letters\",\n",
    "        \"Data Mining and Knowledge Discovery\",\n",
    "        \"IEEE Transactions on Neural Networks\",\n",
    "        \"Pattern Recognition Letters\",\n",
    "        \"Journal of Cryptology\",\n",
    "        \"Network Science\",\n",
    "        \"Annals of Statistics\",\n",
    "    ]\n",
    "\n",
    "    generated_rows = []\n",
    "    for i in range(1, 51):\n",
    "        topic_name = topics[(i - 1) % len(topics)]\n",
    "        domain, field, subfield, topic_label = topic_meta[topic_name]\n",
    "        journal = journals[(i - 1) % len(journals)]\n",
    "        publication_year = 2005 + (i % 20)\n",
    "        cted_by_count = (i * 7) % 200\n",
    "\n",
    "        doi = f\"10.1234/sim.{publication_year}.{i:03d}\"\n",
    "        title = f\"Simulated Study {i} on {topic_name}\"\n",
    "        abstract = (\n",
    "            f\"This simulated article ({i}) discusses methods and experiments related to \"\n",
    "            f\"{topic_name.lower()} with applications in applied mathematics and computer science.\"\n",
    "        )\n",
    "\n",
    "        generated_rows.append(\n",
    "            (\n",
    "                doi,\n",
    "                abstract,\n",
    "                title,\n",
    "                publication_year,\n",
    "                cted_by_count,\n",
    "                journal,\n",
    "                domain,\n",
    "                field,\n",
    "                subfield,\n",
    "                topic_label,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    rows = base_rows + generated_rows\n",
    "\n",
    "    # ВАЖНО: используем DO UPDATE, чтобы заполнить колонки для уже существующих строк\n",
    "    insert_sql = \"\"\"\n",
    "    INSERT INTO articles (\n",
    "        doi,\n",
    "        abstract,\n",
    "        title,\n",
    "        publication_year,\n",
    "        cted_by_count,\n",
    "        journal,\n",
    "        domain,\n",
    "        field,\n",
    "        subfield,\n",
    "        topic\n",
    "    )\n",
    "    VALUES %s\n",
    "    ON CONFLICT (doi) DO UPDATE SET\n",
    "        abstract         = EXCLUDED.abstract,\n",
    "        title            = EXCLUDED.title,\n",
    "        publication_year = EXCLUDED.publication_year,\n",
    "        cted_by_count    = EXCLUDED.cted_by_count,\n",
    "        journal          = EXCLUDED.journal,\n",
    "        domain           = EXCLUDED.domain,\n",
    "        field            = EXCLUDED.field,\n",
    "        subfield         = EXCLUDED.subfield,\n",
    "        topic            = EXCLUDED.topic;\n",
    "    \"\"\"\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        execute_values(cur, insert_sql, rows)\n",
    "    conn.commit()\n",
    "    print(f\"Добавлено/обновлено {len(rows)} записей.\")\n",
    "\n",
    "\n",
    "# ---------- 5. Проверочный вывод ----------\n",
    "def show_data(conn, limit=15):\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT id, doi, title, publication_year, cted_by_count, journal,\n",
    "                   domain, field, subfield, topic\n",
    "            FROM articles\n",
    "            ORDER BY id\n",
    "            LIMIT %s;\n",
    "            \"\"\",\n",
    "            (limit,),\n",
    "        )\n",
    "        for row in cur.fetchall():\n",
    "            print(row)\n",
    "\n",
    "\n",
    "ensure_database()\n",
    "conn = get_target_connection()\n",
    "try:\n",
    "    create_table(conn)\n",
    "    migrate_articles_add_new_columns(conn)\n",
    "    insert_sample_data(conn)\n",
    "    print(\"Первые строки в таблице:\")\n",
    "    show_data(conn, limit=15)\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = get_target_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Запрос #1: Опиши, какие данные есть в нашей базе научных публикаций: диапазон лет, основные домены и поля, какие журналы и конференции встречаются чаще всего.\n",
      "--------------------------------------------------------------------------------\n",
      "{\n",
      "  \"user_intent\": \"describe_dataset\",\n",
      "  \"question_rewrite\": \"Описать структуру и содержимое базы данных научных публикаций, включая диапазон лет, основные домены, поля, а также наиболее часто встречающиеся журналы и конференции.\",\n",
      "  \"requires_classifier\": false,\n",
      "  \"classification_dimension\": null,\n",
      "  \"filters\": [],\n",
      "  \"collector_sql_template\": \"SELECT MIN(publication_year) AS min_year, MAX(publication_year) AS max_year, COUNT(*) AS total_papers, COUNT(DISTINCT domain) AS unique_domains, COUNT(DISTINCT field) AS unique_fields, COUNT(DISTINCT journal) AS unique_journals FROM articles;\",\n",
      "  \"collector_parameters\": {},\n",
      "  \"required_parameters\": [],\n",
      "  \"expected_result_format\": \"table_and_summary\",\n",
      "  \"analyst_instructions\": \"Представить агрегированные метрики: минимальный и максимальный годы публикаций, общее количество публикаций, количество уникальных доменов, полей и журналов. Сформулировать краткий текстовый обзор структуры базы данных.\",\n",
      "  \"steps\": [\n",
      "    {\n",
      "      \"step_id\": \"plan\",\n",
      "      \"description\": \"Шаг 'plan' выполняется агентом Planner.\",\n",
      "      \"agent\": \"Planner\",\n",
      "      \"input_from\": []\n",
      "    },\n",
      "    {\n",
      "      \"step_id\": \"collect\",\n",
      "      \"description\": \"Шаг 'collect' выполняется агентом Collector.\",\n",
      "      \"agent\": \"Collector\",\n",
      "      \"input_from\": [\n",
      "        \"plan\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"step_id\": \"analyze\",\n",
      "      \"description\": \"Шаг 'analyze' выполняется агентом Analyst.\",\n",
      "      \"agent\": \"Analyst\",\n",
      "      \"input_from\": [\n",
      "        \"collect\"\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\\n=== Первые строки результата Collector'а ===\n",
      "[(2005, 2024, 56, 5, 12, 16)]\n",
      "\n",
      "================================================================================\n",
      "Запрос #2: Построй динамику числа публикаций по теме deep learning с 2010 по 2024 год и прокомментируй тренды.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "PlannerOutput не прошёл валидацию Pydantic: 1 validation error for PlannerOutput\nfilters\n  Input should be a valid list [type=list_type, input_value=\"Годы публика...ые с 'deep learning'\", input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/list_type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 256\u001b[0m, in \u001b[0;36mcall_planner_llm\u001b[1;34m(user_query)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PlannerOutput\u001b[38;5;241m.\u001b[39mmodel_validate(obj)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:716\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[1;34m(cls, obj, strict, extra, from_attributes, context, by_alias, by_name)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[0;32m    712\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    713\u001b[0m         code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidate-by-alias-and-name-false\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    714\u001b[0m     )\n\u001b[1;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_validator__\u001b[38;5;241m.\u001b[39mvalidate_python(\n\u001b[0;32m    717\u001b[0m     obj,\n\u001b[0;32m    718\u001b[0m     strict\u001b[38;5;241m=\u001b[39mstrict,\n\u001b[0;32m    719\u001b[0m     extra\u001b[38;5;241m=\u001b[39mextra,\n\u001b[0;32m    720\u001b[0m     from_attributes\u001b[38;5;241m=\u001b[39mfrom_attributes,\n\u001b[0;32m    721\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m    722\u001b[0m     by_alias\u001b[38;5;241m=\u001b[39mby_alias,\n\u001b[0;32m    723\u001b[0m     by_name\u001b[38;5;241m=\u001b[39mby_name,\n\u001b[0;32m    724\u001b[0m )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for PlannerOutput\nfilters\n  Input should be a valid list [type=list_type, input_value=\"Годы публика...ые с 'deep learning'\", input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/list_type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mЗапрос #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m planner_output \u001b[38;5;241m=\u001b[39m call_planner_llm(q)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Красивый вывод результата Planner’а\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     10\u001b[0m     json\u001b[38;5;241m.\u001b[39mdumps(\n\u001b[0;32m     11\u001b[0m         planner_output\u001b[38;5;241m.\u001b[39mmodel_dump(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     )\n\u001b[0;32m     15\u001b[0m )\n",
      "Cell \u001b[1;32mIn[52], line 258\u001b[0m, in \u001b[0;36mcall_planner_llm\u001b[1;34m(user_query)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PlannerOutput\u001b[38;5;241m.\u001b[39mmodel_validate(obj)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlannerOutput не прошёл валидацию Pydantic: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: PlannerOutput не прошёл валидацию Pydantic: 1 validation error for PlannerOutput\nfilters\n  Input should be a valid list [type=list_type, input_value=\"Годы публика...ые с 'deep learning'\", input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/list_type"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i, q in enumerate(EXAMPLE_QUERIES, start=1):\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Запрос #{i}: {q}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        planner_output = call_planner_llm(q)\n",
    "\n",
    "        # Красивый вывод результата Planner’а\n",
    "        print(\n",
    "            json.dumps(\n",
    "                planner_output.model_dump(mode=\"json\"),\n",
    "                ensure_ascii=False,\n",
    "                indent=2,\n",
    "            )\n",
    "        )\n",
    "        # 2. Словари значений из БД\n",
    "        #    Здесь предполагается, что у тебя уже есть открытое соединение conn\n",
    "        #    к нужной БД (например, через get_target_connection()).\n",
    "        #    Если таблица называется 'articles', то поставь table_name='articles'.\n",
    "        dbtool = DBTool(conn, table_name=\"articles\")  # или \"articles\"\n",
    "        possible_values = dbtool.load_all_distinct_values()\n",
    "        \n",
    "        classifier_output = None\n",
    "        rows = None\n",
    "        \n",
    "        if planner_output.requires_classifier:\n",
    "            # 3. Собираем контекст и вызываем Classifier\n",
    "            classifier_ctx = build_classifier_context(user_query, planner_output, possible_values)\n",
    "            classifier_output = call_classifier_llm(classifier_ctx)\n",
    "        \n",
    "            print(\"\\\\n=== ClassifierOutput ===\")\n",
    "            print(classifier_output.model_dump(mode=\"json\"))\n",
    "        \n",
    "            # 4. Collector: выполняем SQL с подставленными параметрами\n",
    "            rows = run_collector_query(\n",
    "                conn,\n",
    "                planner_output.collector_sql_template,\n",
    "                classifier_output.resolved_parameters,\n",
    "            )\n",
    "        else:\n",
    "            # Если Classifier не нужен, просто выполняем запрос без доп. параметров\n",
    "            rows = run_collector_query(\n",
    "                conn,\n",
    "                planner_output.collector_sql_template,\n",
    "                {},\n",
    "            )\n",
    "        \n",
    "        print(\"\\\\n=== Первые строки результата Collector'а ===\")\n",
    "        print(rows[:5] if rows is not None else None)\n",
    "        print()  # пустая строка между примерами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
